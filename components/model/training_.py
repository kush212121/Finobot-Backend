# -*- coding: utf-8 -*-
"""Normal_Chatting_ChatBot .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KtAQ51U085Huz8T-yJA8C1etw0SpKV22
"""

import nltk
import json
import pickle
import random
import json
import numpy as np
import tensorflow as tf
from itertools import groupby 
from keras.optimizers import SGD
from keras.models import Sequential, load_model
from nltk.stem import WordNetLemmatizer
from tensorflow.keras.optimizers import Adam
from keras.layers import Dense, Activation, Dropout
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional,Dropout

"""#Loading The Json Data"""

intents_file = open('/content/data.json').read()
intents = json.loads(intents_file)

lemmatizer = WordNetLemmatizer()
nltk.download('punkt')
nltk.download('wordnet')

words=[]
classes = []
documents = []
ignore_letters = ['!', '?', ',', '.',"a", "about", "above", "after", "again", "against", "all", "am", "an", "and", "any", "are", "as", "at", "be", "because", "been", "before", "being", "below", "between", "both", "but", "by", "could", "did", "do", "does", "doing", "down", "during", "each", "few", "for", "from", "further", "had", "has", "have", "having", "he", "he'd", "he'll", "he's", "her", "here", "here's", "hers", "herself", "him", "himself", "his", "how", "how's", "i", "i'd", "i'll", "i'm", "i've", "if", "in", "into", "is", "it", "it's", "its", "itself", "let's", "me", "more", "most", "my", "myself", "nor", "of", "on", "once", "only", "or", "other", "ought", "our", "ours", "ourselves", "out", "over", "own", "same", "she", "she'd", "she'll", "she's", "should", "so", "some", "such", "than", "that", "that's", "the", "their", "theirs", "them", "themselves", "then", "there", "there's", "these", "they", "they'd", "they'll", "they're", "they've", "this", "those", "through", "to", "too", "under", "until", "up", "very", "was", "we", "we'd", "we'll", "we're", "we've", "were", "what's", "when", "when's", "where", "where's", "which", "while", "who", "who's", "whom", "why", "why's", "with", "would", "you", "you'd", "you'll", "you're", "you've", "your", "yours", "yourself", "yourselves","'s"]
for intent in intents['intents']:
    for pattern in intent['patterns']:
        #tokenize each word
        word = nltk.word_tokenize(pattern)
        words.extend(word)        
        #add documents in the corpus
        documents.append((word, intent['tag']))
        # add to our classes list
        if intent['tag'] not in classes:
            classes.append(intent['tag'])
print(documents)

# lemmaztize and lower each word and remove duplicates
words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_letters]
words = sorted(list(set(words)))
# sort classes
classes = sorted(list(set(classes)))
# documents = combination between patterns and intents
print (len(documents), "documents")
# classes = intents
print (len(classes), "classes", classes)
# words = all words, vocabulary
print (len(words), "unique lemmatized words", words)
pickle.dump(words,open('words.pkl','wb'))
pickle.dump(classes,open('classes.pkl','wb'))

# create the training data
training = []
# create empty array for the output
output_empty = [0] * len(classes)
# training set, bag of words for every sentence
for doc in documents:
    # initializing bag of words
    bag = []
    # list of tokenized words for the pattern
    word_patterns = doc[0]
    # lemmatize each word - create base word, in attempt to represent related words
    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]
    # create the bag of words array with 1, if word is found in current pattern
    for word in words:
        bag.append(1) if word in word_patterns else bag.append(0)
    # output is a '0' for each tag and '1' for current tag (for each pattern)
    output_row = list(output_empty)
    output_row[classes.index(doc[1])] = 1
    training.append([bag, output_row])
# shuffle the features and make numpy array
random.shuffle(training)
training = np.array(training)
# create training and testing lists. X - patterns, Y - intents
train_x = list(training[:,0])
train_y = list(training[:,1])
print(train_x)
print("Training data is created")
print(len(train_x))

"""# Creating Model"""

model = Sequential()
model.add(Dense(120, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(60, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(60, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(len(train_y[0]), activation='softmax'))
# Compiling model. SGD with Nesterov accelerated gradient gives good results for this model
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
model.summary()
#Training and saving the model 
history = model.fit(np.array(train_x), np.array(train_y), epochs=100, verbose=1 , validation_split=0.1)

!rm -rf /content/Chatbot

"""# KERAS HYPERTUNER"""

!rm -rf /content/Chatbot

!pip install -q -U keras-tuner
import kerastuner as kt
import tensorflow
from tensorflow import keras
def model_builder(hp):
  model = Sequential()
  hp_units = hp.Int('units', min_value=50, max_value=150, step=10)
  model.add(Dense(hp_units, input_shape=(len(train_x[0]),), activation='relu'))
  model.add(Dropout(0.5))
  # model.add(Dense(60, activation='relu'))
  # model.add(Dropout(0.3))
  model.add(Dense(32, activation='relu'))
  model.add(Dense(len(train_y[0]), activation='softmax'))
  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
  model.compile(
      loss='categorical_crossentropy', 
      optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),
       metrics=['accuracy'])


  return model


tuner = kt.Hyperband(model_builder,
                     objective='val_accuracy',
                     max_epochs=300,
                     factor=20,
                     directory='/content/Chatbot',
                     project_name='Chatbot')

stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)
tuner.search(np.array(train_x), 
             np.array(train_y), 
             epochs=200, 
             verbose=1,
             shuffle=True,
             validation_split=0.1, 
             callbacks=[stop_early]
             )

best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]
print(best_hps)
best_model = tuner.hypermodel.build(best_hps)
history = best_model.fit(
    np.array(train_x), 
    np.array(train_y),
    epochs=200,
    verbose=1,
    validation_split=0.1)

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.show()

best_model.save('Chatbot_Best')
converter = tf.lite.TFLiteConverter.from_saved_model('/content/Chatbot_Best') # path to the SavedModel directory
tflite_model = converter.convert()

with open('model.tflite','wb') as file:
  file.write(tflite_model)

def remove_all_consecutive(str1): 
	result_str = [] 
	for (key,group) in groupby(str1): 
		result_str.append(key) 

	return ''.join(result_str)

intents = json.loads(open('/content/data.json').read())
words = pickle.load(open('words.pkl','rb'))
classes = pickle.load(open('classes.pkl','rb'))
def clean_up_sentence(sentence):
    # tokenize the pattern - splitting words into array
    sentence_words = nltk.word_tokenize(sentence)
    # stemming every word - reducing to base form
    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]
    return sentence_words
# return bag of words array: 0 or 1 for words that exist in sentence
def bag_of_words(sentence, words, show_details=True):
    # tokenizing patterns
    sentence_words = clean_up_sentence(sentence)
    # bag of words - vocabulary matrix
    bag = [0]*len(words)  
    for s in sentence_words:
        for i,word in enumerate(words):
            if word == s: 
                # assign 1 if current word is in the vocabulary position
                bag[i] = 1
                if show_details:
                    print ("found in bag: %s" % word)
    return(np.array(bag))

from keras.models import load_model
# model = load_model('chatbot.h5')
import json
import random
intents = json.loads(open('/content/data.json').read())
words = pickle.load(open('words.pkl','rb'))
classes = pickle.load(open('classes.pkl','rb'))
def clean_up_sentence(sentence):
    # tokenize the pattern - splitting words into array
    sentence_words = nltk.word_tokenize(sentence)
    # stemming every word - reducing to base form
    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]
    return sentence_words
# return bag of words array: 0 or 1 for words that exist in sentence
def bag_of_words(sentence, words, show_details=True):
    # tokenizing patterns
    sentence_words = clean_up_sentence(sentence)
    # bag of words - vocabulary matrix
    bag = [0]*len(words)  
    for s in sentence_words:
        for i,word in enumerate(words):
            if word == s: 
                # assign 1 if current word is in the vocabulary position
                bag[i] = 1
                if show_details:
                    print ("found in bag: %s" % word)
    return(np.array(bag))
    
# def predict_class(sentence):
#     sentence=remove_all_consecutive(sentence)
#     # filter below  threshold predictions
#     p = bag_of_words(sentence, words,show_details=False)
#     res = model.predict(np.array([p]))[0]
#     print(res)
#     ERROR_THRESHOLD = 0.50
#     results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]
#     # sorting strength probability
#     results.sort(key=lambda x: x[1], reverse=True)
#     return_list = []
#     for r in results:
#         if r[1]<=0.50:
#           return_list.append({"intent": 'unknown_token', "probability": str(r[1])})
#         else:
#           return_list.append({"intent": classes[r[0]], "probability": str(r[1])})
#     return return_list

# def getResponse(ints, intents_json):
#     tag = ints[0]['intent']
#     print(tag)
#     list_of_intents = intents_json['intents']
#     print(list_of_intents)
#     for i in list_of_intents:
#         if(i['tag']== tag):
#             result = random.choice(i['patterns'])
#     return result

"""# TFLITE MODEL - Recontruction"""

# confidence_threshold = 0.3
# intents = json.loads(open('/content/data.json').read())
# words = pickle.load(open('words.pkl','rb'))
# classes = pickle.load(open('classes.pkl','rb'))
# p=bag_of_words(sentence,words,show_details=False)

# interpreter = tf.lite.Interpreter(model_path="/content/model.tflite")
# interpreter.allocate_tensors()
# input_details = interpreter.get_input_details()
# output_details = interpreter.get_output_details()

# print(input_details)
# print(output_details)

# sentence='Hii'
# p = bag_of_words(sentence, words,show_details=False)
# input_shape = input_details[0]['shape']
# input_data =  np.array([p], dtype=np.float32)
# print(input_data)
# print(input_data.shape)
# print(input_data.dtype)

# interpreter.set_tensor(input_details[0]['index'], input_data)
# interpreter.invoke()
# output_details = interpreter.get_output_details() 
# output_data = interpreter.get_tensor(output_details[0]['index'])
# results = np.squeeze(output_data)
# print(results)
# pred = np.argmax(results)
# print(pred)
# print(classes[pred])

def model_predict(sentence):
  confidence_threshold = 0.5
  unknown_intent = "unknown_token"
  sentence=remove_all_consecutive(sentence)
  p=bag_of_words(sentence,words,show_details=False)
  interpreter = tf.lite.Interpreter(model_path="/content/model.tflite")
  interpreter.allocate_tensors()
  
  input_details = interpreter.get_input_details()
  output_details = interpreter.get_output_details()

  #input_shape = input_details[0]['shape']
  input_data =  np.array([p], dtype=np.float32)
  interpreter.set_tensor(input_details[0]['index'], input_data)
  interpreter.invoke()
  output_details = interpreter.get_output_details() 
  output_data = interpreter.get_tensor(output_details[0]['index'])
  results = np.squeeze(output_data)
  print(results)
  max_probability = np.argmax(results)
  print(results[max_probability])

  if results[max_probability] < confidence_threshold:
    print(results[max_probability])
    return unknown_intent
  else:
    return classes[max_probability]

intents=model_predict('Thanks')
print(intents)

